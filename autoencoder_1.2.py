# -*- coding: utf-8 -*-
"""autoencoder.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/187c471W21Tm6ksLzndO8nXqf-Lkmai0u

# ðŸ‘– Autoencoders on Fashion MNIST

In this notebook, we'll walk through the steps required to train your own autoencoder on the fashion MNIST dataset.
"""

!pip install pillow_heif

# Commented out IPython magic to ensure Python compatibility.
# %load_ext autoreload
# %autoreload 2

import numpy as np
import matplotlib.pyplot as plt

from tensorflow.keras import layers, models, datasets, callbacks
import tensorflow.keras.backend as K
from tensorflow.image import resize as tfresize

import pillow_heif
import os

def display(images, n=10, m=1):
  fig=plt.figure()
  fig.set_figheight(15)
  fig.set_figwidth(15)
  for i in range(n):
      plt.subplot(m, n, i+1)
      plt.xticks([])
      plt.yticks([])
      plt.imshow(images[i], cmap=plt.cm.binary)
  plt.show()

from google.colab import drive
drive.mount("/content/drive/")

"""## 0. Parameters <a name="parameters"></a>"""

IMAGE_SIZE = 32
CHANNELS = 3
BATCH_SIZE = 100
BUFFER_SIZE = 1000
VALIDATION_SPLIT = 0.2
EMBEDDING_DIM = 2
EPOCHS = 50

"""## 1. Prepare the data <a name="prepare"></a>"""

# Load the data
(x_train, y_train), (x_test, y_test) = datasets.fashion_mnist.load_data()

filepath = "/content/drive/My Drive/Computational Creativity/hw1/real/"
newpath = "/content/drive/My Drive/Computational Creativity/hw1/numpy/"
images = []

# Convert Files from HEIC/JPG to PNG
for filename in os.listdir(filepath):
    print(filename, end="   ")
    if (filename.endswith("heic") or filename.endswith("HEIC")) and pillow_heif.is_supported(filepath+filename):
        heif_file = pillow_heif.open_heif(filepath+filename)
        img = np.asarray(heif_file)
        filename = filename.replace(".HEIC", ".png")
        filename = filename.replace(".heic", ".png")
        # img = img[::img.shape[0]//1024+1, ::img.shape[1]//1024+1]
        newimg = tfresize(img, [1024, 1024]);
        print(newimg.shape)
        plt.imsave(newpath+filename, img)

!mkdir "/content/drive/My Drive/Computational Creativity/DATA ECSE 6965/PNG"

filepath = "/content/drive/My Drive/Computational Creativity/DATA ECSE 6965/"
images = []
for filename in os.listdir(filepath):
    print(filename)
    img = plt.imread(filepath+filename)
    img = img[::img.shape[0]//IMAGE_SIZE+1, ::img.shape[1]//IMAGE_SIZE+1]
    images.append(img.copy())

filepath = "/content/drive/My Drive/Computational Creativity/hw1/"
images = []
for filename in os.listdir(filepath):

display(images, 15)

np_images = np.asarray(images)

plt.imsave()

print(len(np_images))

x_train = np_images[0:450]
x_test = np_images[450:537]

print(x_train.shape)
print(x_test.shape)

# Preprocess the data


def preprocess(imgs):
    """
    Normalize and reshape the images
    """
    imgs = imgs.astype("float32") / 255.0
    # imgs = np.pad(imgs, ((0, 0), (2, 2), (2, 2), (0,0)), constant_values=0.0)
    # imgs = np.expand_dims(imgs, -1)
    return imgs


x_train = preprocess(x_train)
x_test = preprocess(x_test)

# Show some items of clothing from the training set
display(x_train, 10)

"""## 2. Build the autoencoder <a name="build"></a>"""

# Encoder
encoder_input = layers.Input(
    shape=(IMAGE_SIZE, IMAGE_SIZE, CHANNELS), name="encoder_input"
)
x = layers.Conv2D(32, (3, 3), strides=2, activation="relu", padding="same")(
    encoder_input
)
x = layers.Conv2D(64, (3, 3), strides=2, activation="relu", padding="same")(x)
x = layers.Conv2D(128, (3, 3), strides=2, activation="relu", padding="same")(x)
shape_before_flattening = K.int_shape(x)[1:]  # the decoder will need this!

x = layers.Flatten()(x)
encoder_output = layers.Dense(EMBEDDING_DIM, name="encoder_output")(x)

encoder = models.Model(encoder_input, encoder_output)
encoder.summary()

# Decoder
decoder_input = layers.Input(shape=(EMBEDDING_DIM,), name="decoder_input")
x = layers.Dense(np.prod(shape_before_flattening))(decoder_input)
x = layers.Reshape(shape_before_flattening)(x)
x = layers.Conv2DTranspose(
    128, (3, 3), strides=2, activation="relu", padding="same"
)(x)
x = layers.Conv2DTranspose(
    64, (3, 3), strides=2, activation="relu", padding="same"
)(x)
x = layers.Conv2DTranspose(
    32, (3, 3), strides=2, activation="relu", padding="same"
)(x)
decoder_output = layers.Conv2D(
    CHANNELS,
    (3, 3),
    strides=1,
    activation="sigmoid",
    padding="same",
    name="decoder_output",
)(x)

decoder = models.Model(decoder_input, decoder_output)
decoder.summary()

# Autoencoder
autoencoder = models.Model(
    encoder_input, decoder(encoder_output)
)  # decoder(encoder_output)
autoencoder.summary()

"""## 3. Train the autoencoder <a name="train"></a>"""

# Compile the autoencoder
autoencoder.compile(optimizer="adam", loss="binary_crossentropy")

# Create a model save checkpoint
model_checkpoint_callback = callbacks.ModelCheckpoint(
    filepath="./checkpoint",
    save_weights_only=False,
    save_freq="epoch",
    monitor="loss",
    mode="min",
    save_best_only=True,
    verbose=0,
)
tensorboard_callback = callbacks.TensorBoard(log_dir="./logs")

autoencoder.fit(
    x_train,
    x_train,
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    shuffle=True,
    validation_data=(x_test, x_test),
    callbacks=[model_checkpoint_callback, tensorboard_callback],
)

# Save the final models
autoencoder.save("./models/autoencoder")
encoder.save("./models/encoder")
decoder.save("./models/decoder")

"""## 4. Reconstruct using the autoencoder <a name="reconstruct"></a>"""

n_to_predict = 5000
example_images = x_test[:n_to_predict]
example_labels = y_test[:n_to_predict]

predictions = autoencoder.predict(example_images)

print("Example real clothing items")
display(example_images)
print("Reconstructions")
display(predictions)

"""## 5. Embed using the encoder <a name="encode"></a>"""

# Encode the example images
embeddings = encoder.predict(example_images)

# Some examples of the embeddings
print(embeddings[:10])

# Show the encoded points in 2D space
figsize = 8

plt.figure(figsize=(figsize, figsize))
plt.scatter(embeddings[:, 0], embeddings[:, 1], c="black", alpha=0.5, s=3)
plt.show()

# Colour the embeddings by their label (clothing type - see table)
example_labels = y_test[:n_to_predict]

figsize = 8
plt.figure(figsize=(figsize, figsize))
plt.scatter(
    embeddings[:, 0],
    embeddings[:, 1],
    cmap="rainbow",
    # c=example_labels,
    alpha=0.8,
    s=3,
)
plt.colorbar()
plt.show()

"""## 6. Generate using the decoder <a name="decode"></a>"""

# Get the range of the existing embeddings
mins, maxs = np.min(embeddings, axis=0), np.max(embeddings, axis=0)

# Sample some points in the latent space
grid_width, grid_height = (6, 3)
sample = np.random.uniform(
    mins, maxs, size=(grid_width * grid_height, EMBEDDING_DIM)
)

# Decode the sampled points
reconstructions = decoder.predict(sample)

# Draw a plot of...
figsize = 8
plt.figure(figsize=(figsize, figsize))

# ... the original embeddings ...
plt.scatter(embeddings[:, 0], embeddings[:, 1], c="black", alpha=0.5, s=2)

# ... and the newly generated points in the latent space
plt.scatter(sample[:, 0], sample[:, 1], c="#00B0F0", alpha=1, s=40)
plt.show()

# Add underneath a grid of the decoded images
fig = plt.figure(figsize=(figsize, grid_height * 2))
fig.subplots_adjust(hspace=0.4, wspace=0.4)

for i in range(grid_width * grid_height):
    ax = fig.add_subplot(grid_height, grid_width, i + 1)
    ax.axis("off")
    ax.text(
        0.5,
        -0.35,
        str(np.round(sample[i, :], 1)),
        fontsize=10,
        ha="center",
        transform=ax.transAxes,
    )
    ax.imshow(reconstructions[i, :, :], cmap="Greys")

# Colour the embeddings by their label (clothing type - see table)
figsize = 12
grid_size = 15
plt.figure(figsize=(figsize, figsize))
plt.scatter(
    embeddings[:, 0],
    embeddings[:, 1],
    cmap="rainbow",
    # c=example_labels,
    alpha=0.8,
    s=300,
)
plt.colorbar()

x = np.linspace(min(embeddings[:, 0]), max(embeddings[:, 0]), grid_size)
y = np.linspace(max(embeddings[:, 1]), min(embeddings[:, 1]), grid_size)
xv, yv = np.meshgrid(x, y)
xv = xv.flatten()
yv = yv.flatten()
grid = np.array(list(zip(xv, yv)))

reconstructions = decoder.predict(grid)
# plt.scatter(grid[:, 0], grid[:, 1], c="black", alpha=1, s=10)
plt.show()

fig = plt.figure(figsize=(figsize, figsize))
fig.subplots_adjust(hspace=0.4, wspace=0.4)
for i in range(grid_size**2):
    ax = fig.add_subplot(grid_size, grid_size, i + 1)
    ax.axis("off")
    ax.imshow(reconstructions[i, :, :], cmap="Greys")